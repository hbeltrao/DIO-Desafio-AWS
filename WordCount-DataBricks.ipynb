{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import lower, length, split\n\n# to install nltk into databricks cluster, go to the cluster > libraries and add nltk\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Creation of the spark session\nspark = SparkSession.builder.enableHiveSupport().getOrCreate()\nsc = spark._sc\nsc = spark.sparkContext\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8002a378-05ba-48d6-8391-61a5ddb7c493"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Loading the data into a RDD to better use spark functions\ndata = sc.textFile(\"/FileStore/tables/sherlock.txt\").flatMap(lambda line: line.split(\" \"))\n\n# Tokenizing the data to have each word as an element\nwords = data.flatMap(word_tokenize)\n\n# Setting the stopwords and punctuations to be removed from the data\nstop_words = set(stopwords.words('english') +  [\",\", \".\", \"“\", \"”\", \"’\", \"?\", \"‘\", \"!\", \";\"])\n\n# Removing all stopwords and punctuations from the data\nbook_words = words.filter(lambda x: x.lower() not in stop_words)\n\n# Creating the dictionary with all the words and their respective count, sorted in descending order\nwordCounts = book_words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b).sortBy(lambda a:a[1], ascending=False)\n\n# Saving final data into file as parquet (coalesce(1) is to return only one parquet file)\nwordCounts.coalesce(1).saveAsTextFile(\"/FileStore/teste/resultado2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb871001-ab5c-42c3-85d9-d98556e59a59"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"WordCount v2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3122605201744560}},"nbformat":4,"nbformat_minor":0}
